{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GEO371T/GEO391 - Introduction to Machine Learning for Geosciences\n",
    "### Simple Descriptive and Inferential Statistics \n",
    "#### What should you take from this workflow?\n",
    "* Become familiar with how to implement various statistical measures given well, flower and housing data\n",
    "\n",
    "__Well data:__\n",
    "* This is part of the Bakken unconventional reservoir dataset series, but will be left for homework\n",
    "\n",
    "__Flower data:__\n",
    "* This dataset is from the sklearn library, and has a series of predictive features that describe several types of iris flowers\n",
    "\n",
    "__Housing data:__\n",
    "* This dataset is also from the sklearn library, it is based on the Boston Housing problem, and has a series of predictive features (e.g. Crime, Owner Age, Distance to Employment Centers,...) that can be used to predict the dataset's target feature: Median value of owner-occupied homes in units $1000s\n",
    "\n",
    "For more info on the flower and housing datasets, please visit: https://scikit-learn.org/stable/datasets/index.html\n",
    "\n",
    "In this workflow, we will show how to describe and generating statistical measures for both the flower and housing datasets. Some of this may be repeat from the Lab02 labs, but there will be some new methods and procedures laid out here in this lab.\n",
    "\n",
    "Before going through this workflow, let's first load in our libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate statistics \n",
    "\n",
    "Ref: /pystatsml-master/statistics/\n",
    "\n",
    "Basics univariate statistics are required to explore dataset:\n",
    "\n",
    "- Discover associations between a variable of interest and potential predictors. It is strongly recommended to start with simple univariate methods before moving to complex multivariate predictors. \n",
    "\n",
    "- Assess the prediction performances of machine learning predictors.\n",
    "\n",
    "- Most of the univariate statistics are based on the linear model which is one of the main model in machine learning.\n",
    "\n",
    "\n",
    "## Estimators of the main statistical measures\n",
    "\n",
    "\n",
    "### Expected Value\n",
    "\n",
    "Properties of the expected value operator $\\operatorname{E}(\\cdot)$ of a random variable $X$\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "    E(X + c) &= E(X) + c \\\\ \n",
    "    E(X + Y) &= E(X) + E(Y) \\\\ \n",
    "    E(aX)    &= a E(X)\n",
    "\\end{align}\n",
    "\n",
    "$$\n",
    "    E(X) = \\frac{1}{n} \\sum_i p_ix_i\n",
    "$$\n",
    "\n",
    "### Mean\n",
    "\n",
    "The estimator $\\bar{x}$ on a sample of size $n$: $x = x_1, ..., x_n$ is given by\n",
    "$$\n",
    "    \\bar{x} = \\frac{1}{n} \\sum_i x_i\n",
    "$$\n",
    "\n",
    "$\\bar{x}$ is itself a random variable with properties:\n",
    "\n",
    "- $E(\\bar{x}) = \\bar{x}$,\n",
    "\n",
    "- $Var(\\bar{x}) = \\frac{Var(X)}{n}$.\n",
    "\n",
    "### Variance\n",
    "\n",
    "$$\n",
    "    Var(X) = E((X - E(X))^2) =  E(X^2) - (E(X))^2\n",
    "$$\n",
    "\n",
    "The estimator is\n",
    "$$\n",
    "    \\sigma_x^2 = \\frac{1}{n-1} \\sum_i (x_i - \\bar{x})^2\n",
    "$$\n",
    "\n",
    "Note here the subtracted 1 degree of freedom (df) in the divisor. In standard statistical practice, $df=1$ provides an unbiased estimator of the variance of a hypothetical infinite population. With $df=0$ it instead provides a maximum likelihood estimate of the variance for normally distributed variables.\n",
    "\n",
    "### Standard deviation\n",
    "\n",
    "$$\n",
    "    Std(X) = \\sqrt{Var(X)}\n",
    "$$\n",
    "\n",
    "The estimator is simply $\\sigma_x = \\sqrt{\\sigma_x^2}$.\n",
    "\n",
    "### Covariance\n",
    "\n",
    "$$\n",
    "    Cov(X, Y) = E((X - E(X))(Y - E(Y))) =  E(XY) - E(X)E(Y).\n",
    "$$\n",
    "\n",
    "Properties: \n",
    "$$\n",
    "    \\operatorname{Cov}(X, X) = \\operatorname{Var}(X)\\\\\n",
    "    \\operatorname{Cov}(X, Y) = \\operatorname{Cov}(Y, X)\\\\\n",
    "    \\operatorname{Cov}(cX, Y) = c \\operatorname{Cov}(X, Y)\\\\\n",
    "    \\operatorname{Cov}(X+c, Y) = \\operatorname{Cov}(X, Y)\\\\\n",
    "$$\n",
    "\n",
    "The estimator with $df=1$ is\n",
    "$$\n",
    "    \\sigma_{xy} = \\frac{1}{n-1} \\sum_i (x_i - \\bar{x}) (y_i - \\bar{y}).\n",
    "$$\n",
    "\n",
    "### Correlation\n",
    "\n",
    "$$\n",
    "    Cor(X, Y) = \\frac{Cov(X, Y)}{Std(X)Std(Y)}\n",
    "$$\n",
    "\n",
    "The estimator is\n",
    "$$\n",
    "    \\rho_{xy} = \\frac{\\sigma_{xy}}{\\sigma_{x} \\sigma_{y}}.\n",
    "$$\n",
    "\n",
    "### Standard Error (SE) \n",
    "\n",
    "The standard error (SE) is the standard deviation (of the sampling distribution) of a statistic: \n",
    "$$\n",
    "    SE(X) = \\frac{Std(X)}{\\sqrt{n}}.\n",
    "$$\n",
    "\n",
    "It is most commonly considered for the mean with the estimator\n",
    "\n",
    "\\begin{align}\n",
    "SE(x) &= Std(X) = \\sigma_{\\bar{x}}\\\\\n",
    "      &= \\frac{\\sigma_x}{\\sqrt{n}}.\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all essential libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np                              # ndarrys for gridded data\n",
    "import pandas as pd                             # DataFrames for tabular data\n",
    "import matplotlib.pyplot as plt                 # For plotting\n",
    "import seaborn as sns                           # For matrix scatter plots\n",
    "from scipy import stats                         # Summary statistics and statistical methods\n",
    "sns.set(font_scale=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "- Generate 2 random samples: $x \\sim N(1.78, 0.4)$ and $y \\sim N(1.6, 0.4)$, both of size 10.\n",
    "\n",
    "- Compute $\\bar{x}, \\sigma_x, \\sigma_{xy}$ (`xbar, xvar, xycov`) using numpy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(43)\n",
    "N=100000  # number of random draws\n",
    "\n",
    "y1=np.zeros(N)\n",
    "y2=np.zeros(N)\n",
    "for i in range(0,N):\n",
    "    y1[i]=(np.random.normal(1.78,0.4))\n",
    "    y2[i]=(np.random.normal(1.78,0.2))\n",
    "plt.hist(y1,100)\n",
    "plt.hist(y2,100)\n",
    "plt.title('Normal Distributions')\n",
    "plt.show()\n",
    "\n",
    "print('mean y1 = ', np.mean(y1))\n",
    "print('mean y2 = ', np.mean(y2))\n",
    "print('variance y1 = ', np.var(y1) )\n",
    "print('variance y2 = ', np.var(y2) )\n",
    "print('covariance y1, y1 = ', np.cov(y1,y2))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing a probablity curve\n",
    "xx=np.linspace(-4,4,100)\n",
    "p_y1=stats.norm.pdf(xx,0, 1)  # N[0,1]\n",
    "\n",
    "plt.plot(xx,p_y1,'k')\n",
    "plt.fill_between(xx, 0, p_y1, interpolate=True,where=(xx<=stats.norm.ppf(0.25)))\n",
    "plt.title('Percentile shaded')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Exponential Distribition\n",
    "y_Lap=np.zeros(N)\n",
    "for i in range(0,N):\n",
    "    y_Lap[i]=(np.random.laplace(1.78,0.4))\n",
    "\n",
    "plt.hist(y_Lap,100)\n",
    "plt.title('Laplace Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Inferential statistics \n",
    "\n",
    "Here we focus on hypothesis testing. Typically we make the null hypothesis that there is no relationship between two samples. The relationship could be that the populations means are different (e.g. in the t-test) or some other statistical estimator. We then ask the question: What is the probability of observing this statistical estimator given the null hypothesis? The estimator follows a distribution under the null hypothesis so that we can draw a precise conclusion in terms of probability.\n",
    "\n",
    "Suppose for example we have a sample of sand grains from the beach and a sample of river rocks and we would like to know if the rocks and sand originated from the same location. We could compute the mean grain size (which is a crude proxy for energy of the environment and thus location) and ask the question, do these samples have the same mean grain size? We make the null hypothesis that they do have the same mean, $\\mu_1 = \\mu_2$. Under this assumption, the null hypothesis follows a t-distribution. How this is derived is beyond the scope of the class.\n",
    "\n",
    "Typically, a p-value less than 0.05 (typically â‰¤ 0.05) is considered statistically significant. It indicates strong evidence against the null hypothesis, as there is less than a 5% probability the null hypothesis is correct (and the results are random). Therefore, we reject the null hypothesis, and accept the alternative hypothesis.\n",
    "Here our null hypothesis is that the two time series have the same mean!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ttest_ind is a two-sided test for the null hypothesis that 2 independent samples\n",
    "# have identical average (expected) values.\n",
    "\n",
    "#Note: the t-test should be used when the mean's are normally distributed. Under the central limit theorem,\n",
    "#this is true with a large sample size.\n",
    "\n",
    "#The Student t-test.\n",
    "t_statistic, p_value = stats.ttest_ind(y1, y2)\n",
    "\n",
    "print (\"t-statistic = \", t_statistic)\n",
    "print (\"p-value = \", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again lets do a trivial example to make sure we know what's going on there. \n",
    "import scipy.stats as stats\n",
    "\n",
    "# Generate two random datasets with a normal distribution and the same mean.\n",
    "d1 = stats.norm.rvs(size=100000)\n",
    "d2 = stats.norm.rvs(size=100000)\n",
    "\n",
    "# ttest_ind is a two-sided test for the null hypothesis that 2 independent samples\n",
    "# have identical average (expected) values.\n",
    "t_statistic, p_value = stats.ttest_ind(d1, d2)\n",
    "print (\"t-statistic = \", t_statistic)\n",
    "print (\"p-value = \", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another Example\n",
    "from scipy.stats import norm    #NOTE THE DIFFERENCE HERE\n",
    "mu = 0 # mean\n",
    "variance = 2 #variance\n",
    "sigma = np.sqrt(variance) #standard deviation\",\n",
    "x = np.linspace(mu-3*variance,mu+3*variance, 100)   # covers \n",
    "plt.plot(x, norm.pdf(x, mu, sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first start with the Boston housing dataset, let's split it into x and y components, and check the shape of each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_boston(return_X_y=True)\n",
    "print('X Data Shape, with shape:', X.shape)\n",
    "print('y Data Shape, with shape:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are 13 different kinds of predictive features within the X dataset now, and we have our target feature y, that resembles the Median value of owner-occupied homes in $1000s. With that done, let's plot each feature as a function of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "size = 7\n",
    "\n",
    "plt.subplot(3, 5, 1)\n",
    "plt.plot(X[:, 0])\n",
    "plt.ylabel('CRIM per capita crime rate by town', size=size)\n",
    "plt.xlabel('Number of Data Points')\n",
    "\n",
    "plt.subplot(3, 5, 2)\n",
    "plt.plot(X[:, 1])\n",
    "plt.ylabel('ZN proportion of residential land zoned for lots over 25,000 sq.ft.', size=size)\n",
    "plt.xlabel('Number of Data Points')\n",
    "\n",
    "plt.subplot(3, 5, 3)\n",
    "plt.plot(X[:, 2])\n",
    "plt.ylabel('INDUS proportion of non-retail business acres per town', size=size)\n",
    "plt.xlabel('Number of Data Points')\n",
    "\n",
    "plt.subplot(3, 5, 4)\n",
    "plt.plot(X[:, 3])\n",
    "plt.ylabel('CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)', size=size)\n",
    "plt.xlabel('Number of Data Points')\n",
    "\n",
    "plt.subplot(3, 5, 5)\n",
    "plt.plot(X[:, 4])\n",
    "plt.ylabel('NOX nitric oxides concentration (parts per 10 million)', size=size)\n",
    "plt.xlabel('Number of Data Points')\n",
    "\n",
    "plt.subplot(3, 5, 6)\n",
    "plt.plot(X[:, 5])\n",
    "plt.ylabel('RM average number of rooms per dwelling', size=size)\n",
    "plt.xlabel('Number of Data Points')\n",
    "\n",
    "plt.subplot(3, 5, 7)\n",
    "plt.plot(X[:, 6])\n",
    "plt.ylabel('AGE proportion of owner-occupied units built prior to 1940', size=size)\n",
    "plt.xlabel('Number of Data Points')\n",
    "\n",
    "plt.subplot(3, 5, 8)\n",
    "plt.plot(X[:, 7])\n",
    "plt.ylabel('DIS weighted distances to five Boston employment centres', size=size)\n",
    "plt.xlabel('Number of Data Points')\n",
    "\n",
    "plt.subplot(3, 5, 9)\n",
    "plt.plot(X[:, 8])\n",
    "plt.ylabel('RAD index of accessibility to radial highways', size=size)\n",
    "plt.xlabel('Number of Data Points')\n",
    "\n",
    "plt.subplot(3, 5, 10)\n",
    "plt.plot(X[:, 9])\n",
    "plt.ylabel('TAX full-value property-tax rate per $10,000', size=size)\n",
    "plt.xlabel('Number of Data Points')\n",
    "\n",
    "plt.subplot(3, 5, 11)\n",
    "plt.plot(X[:, 10])\n",
    "plt.ylabel('PTRATIO pupil-teacher ratio by town', size=size)\n",
    "plt.xlabel('Number of Data Points')\n",
    "\n",
    "plt.subplot(3, 5, 12)\n",
    "plt.plot(X[:, 11])\n",
    "plt.ylabel('B 1000(Bk - 0.63)$^2$ where Bk is the proportion of blacks by town', size=size)\n",
    "plt.xlabel('Number of Data Points')\n",
    "\n",
    "plt.subplot(3, 5, 13)\n",
    "plt.plot(X[:, 12])\n",
    "plt.ylabel('LSTAT % lower status of the population', size=size)\n",
    "plt.xlabel('Number of Data Points')\n",
    "\n",
    "plt.subplot(3, 5, 14)\n",
    "plt.plot(y)\n",
    "plt.ylabel('Median value of owner-occupied homes in $1000s', size=size)\n",
    "plt.xlabel('Number of Data Points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a sense in the type of features we are dealing with, and their shape as we move down the rows of the dataset. Let's use numpy array to compute a feature's std and plot the +/- std alongside the original feature values just for the first feature in order to keep this workflow organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "x_length = np.linspace(0, len(X[:, 0]), num=len(X[:, 0]))\n",
    "\n",
    "feature1_std = X[:, 0].std(axis=0)\n",
    "feature1_std_plus = X[:, 0] + feature1_std\n",
    "feature1_std_minus = X[:, 0] - feature1_std\n",
    "plt.plot(x_length, X[:, 0], 'g-', label='Original Data')\n",
    "plt.plot(x_length, feature1_std_plus, 'b-', label='+ std')\n",
    "plt.plot(x_length, feature1_std_minus, 'r-', label='- std')\n",
    "plt.fill_between(x_length, feature1_std_minus, feature1_std_plus, facecolor='y', alpha=0.4, label='fill between +/- std')\n",
    "plt.ylabel('CRIM, per capita crime rate by town')\n",
    "plt.xlabel('Number of Data Points')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we weren't sure or confident in how our data was collected, we could do something like this. Note that this type of plot would give you the most information if you had several columns of slightly varying data of the same feature (say you had production rates sampled each day over a week across a given well), but for the purpose of this lab, this is sufficient. Now let's generate the mean, P10, and P90 for this given feature. Let's also calculate the STD again to see what the actual value was that we just plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "feature1_mean = X[:, 0].mean(axis=0)  # axis 0 corresponds to calculating std dev over all rows\n",
    "print('AVG:', feature1_mean)\n",
    "# STD\n",
    "feature1_std = X[:, 0].std(axis=0)\n",
    "print('STD:', feature1_std)\n",
    "# P10 i.e. 10% of the data are below P10\n",
    "feature1_p10 = np.percentile(X[:, 0], 10, axis=0)\n",
    "print('P10:', feature1_p10)\n",
    "# P90\n",
    "feature1_p90 = np.percentile(X[:, 0], 90, axis=0)\n",
    "print('P90:', feature1_p90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use the SciPy to perform more statistical measures. We will calculate the Student's t-test, where scipy.stats.ttest_1samp() tests if the population mean of data is likely to be equal to a given value (technically if observations are drawn from a Gaussian distribution of given population mean). It returns the T statistic, and the p-value (see the functionâ€™s help) (from https://scipy-lectures.org/packages/statistics/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_1samp(X[:, 0], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a DataFrame with just the first 5 features in order to generate some statistics using the pandas library, and let's include in the target, y, feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(X[:, :5], columns=['feature1', 'feature2', 'feature3', 'feature4', 'feature5'])\n",
    "df2 = pd.DataFrame(y, columns=['feature_target'])\n",
    "df = pd.concat([df1, df2], axis=1)\n",
    "print(df.shape)\n",
    "df.head() # to view the first 5 rows of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the .describe() to get some statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we have found the mean, std, min, max, and several percentiles for each of the features. Let's now calculate the Pearson and rank correlation coefficient. The Pearson correlation coefficient tells you how well feature xi compares to feature xj (i.e., the quality of fit). The rank correlation coefficient tell you the same think, but with the removal of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = df.corr()\n",
    "rank_correlation, rank_correlation_pval = stats.spearmanr(df)\n",
    "rank_correlation = rank_correlation[:, -1][:-1]\n",
    "rank_correlation_pval = rank_correlation_pval[:, -1][:-1]\n",
    "rank_correlation_scatter, rank_correlation_pval = stats.spearmanr(df)\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(correlation, annot=True, linewidth=0, vmin=-1, square=True)\n",
    "plt.title(\"Correlation Heatmap: Features and Response\", size=14)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "xticks = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'feature_target']\n",
    "yticks = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'feature_target']\n",
    "sns.heatmap(rank_correlation_scatter, annot=True, linewidth=0, vmin=-1, square=True, xticklabels=xticks, yticklabels=yticks)\n",
    "plt.title(\"Rank Correlation Heatmap: Features and Response\", size=14)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move onto the flower dataset, and let's set up the problem similar to the previous dataset. Note that are target data here isn't a series of continuous values rather it is categorical. What that means is that we have alternating values of 0, 1, and 2 here corresponding to a Iris-Setosa flower (value 0), a Iris-Versicolor flower (value 1), and a Iris-Virginica flower (value 2). The four predictive features can be used to help a machine learning model learn and be able to describe an area that differentiates between the various flowers (more on that later in the class). For now, let's load in the data and plot the predictive feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris(return_X_y=False)\n",
    "X = iris.data[:, :]     # Using only the feature found in columns 1 & 2 (i.e., sepal length in cm & sepal width in cm)\n",
    "y = iris.target         # This is an array with values of 0, 1, and 2 corresponding to the type of iris flower\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(X[:, 0])\n",
    "plt.xlabel('Number of Data Points')\n",
    "plt.ylabel('sepal length in cm')\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(X[:, 1])\n",
    "plt.xlabel('Number of Data Points')\n",
    "plt.ylabel('sepal width in cm')\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.plot(X[:, 2])\n",
    "plt.xlabel('Number of Data Points')\n",
    "plt.ylabel('petal length in cm')\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.plot(X[:, 3])\n",
    "plt.xlabel('Number of Data Points')\n",
    "plt.ylabel('petal width in cm')\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.plot(y, 'b+')\n",
    "plt.xlabel('Number of Data Points')\n",
    "plt.ylabel('Flower Type')\n",
    "plt.yticks([0, 1, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some boxplots of the four predictive features. But let's first put all of this into a pandas DataFrame, and get our statistical DataFrame to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(X[:, :], columns=['Sepal_Length_cm', 'Sepal_Width_cm', 'Petal_Length_cm', 'Petal_Width_cm'])\n",
    "df2 = pd.DataFrame(y, columns=['Flower_Type'])\n",
    "df = pd.concat([df1, df2], axis=1)\n",
    "print(df.shape)\n",
    "df.describe() # to view statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column=['Sepal_Length_cm', 'Sepal_Width_cm', 'Petal_Length_cm', 'Petal_Width_cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have a couple of outliers within the Sepal_Width_cm feature (i.e., the circles outside the extended bars). Let's find out a couple of other ways to generate some statistics here, where we will the mean value of Sepal_Length_cm for the 0 value flower type using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sepal_Length_cm_mean_0_flower_type = df[df['Flower_Type'] == 0]['Sepal_Length_cm'].mean()\n",
    "print(Sepal_Length_cm_mean_0_flower_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same thing with the STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sepal_Length_cm_std_0_flower_type = df[df['Flower_Type'] == 0]['Sepal_Length_cm'].std()\n",
    "print(Sepal_Length_cm_std_0_flower_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the P10 and P90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sepal_Length_cm_p10_0_flower_type = np.percentile(df[df['Flower_Type'] == 0]['Sepal_Length_cm'], 10, axis=0)\n",
    "Sepal_Length_cm_p90_0_flower_type = np.percentile(df[df['Flower_Type'] == 0]['Sepal_Length_cm'], 90, axis=0)\n",
    "print('P10: ', Sepal_Length_cm_p10_0_flower_type)\n",
    "print('P90: ', Sepal_Length_cm_p90_0_flower_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a scatter plot across all features with univariate distributions between a feature and itself using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df[['Sepal_Length_cm', 'Sepal_Width_cm', 'Petal_Length_cm', 'Petal_Width_cm']], figsize=(12,6))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a high correlation between Petal_Width_cm and Petal_Length_cm, and that the Sepal_Width_cm distribtion looks Gaussian-like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "### For this assignment, you will be working with real production data from the Bakken resevoir. Complete the following tasks\n",
    "\n",
    "\n",
    "### Descriptive Statistics\n",
    "1. Load the production data from Structured_Bakken_Production_Data.csv using pandas.read_csv()\n",
    "2. Remove Nans\n",
    "3. The first 8 columns are features and the 9th column is the target, cumulative oil production. Compute the mean, variance, 25th peercentile, 50th percentile, and 75th percentile of the features (hint: you can do this with one command).\n",
    "4. Compute the covariance matrix of the features\n",
    "5. Create a box plot across all of the features\n",
    "6. Create a scatter plot across all features with univariate distributions between a feature and itself using pandas\n",
    "\n",
    "\n",
    "### Inferential statistics\n",
    "7. Using the independant T-test (scipy.stats.ttest_ind), and a threshold p = 0.05, reject or do not reject the null hypothesis that the mean of the columns 'max_treat_press' and 'Total Depth' come from the same distribution.\n",
    "8. The 2 sample Komolgorov-Smirnov statistic can be used to quantify the distance between the empirical distributions of two samples. It is defined\n",
    "\n",
    "    $$\n",
    "    D_{n,m} = \\max |F_{1,n}(x)-F_{2,m}(x)|\n",
    "    $$\n",
    "    \n",
    "    Where \n",
    "    $$F_{j,n}(x) = \\frac{1}{n}\\sum_{i=1}^{n}I(x,X_{j,i})$$\n",
    "    \n",
    "    is the empirical cumulative distribution function of a sample with n elements. The function $I(x,X_{j,i})$ is a type of *indicator function*; It takes the value 1 if a sample is less than x and 0 otherwise. In other words F(x) is the sum of the number samples less than or equal to $x$.\n",
    "    \n",
    "    Essentially, the KS statistic is the largest absolute distance between the empirical cumulative distributions. \n",
    "    \n",
    "    Write a function that computes the KS statistic for sample 1 = 'fluid_gal_per_ft'and sample 2 = 'propp_lbs_per_ft'. Then, convert your data into an empirical cumulative distribution function. The function ecdf below is provided for this purpose. Plot the empirical CDFs of both on the same plot. \n",
    "    \n",
    "    We can reject the null hypothesis that the distributions are the same at the  $\\alpha = (1-p) = 0.05$ level if \n",
    "    \n",
    "    $$\n",
    "    D_{n,m} > 1.358\\sqrt{\\frac{n+m}{n*m}}\n",
    "    $$\n",
    "    \n",
    "    where $n$ is the number of samples in distribution 1 and $m$ is the number of samples in distribution 2.\n",
    "    \n",
    "    Can we reject the null hypothesis? \n",
    "    \n",
    "$^1$ Note that the KS test is not necessarily the best measure of difference between distributions. For example, the Wasserstein or Earthmover distance is often used in training sophisticated generative neural networks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this as the x values for plotting and for the function below\n",
    "x = np.linspace(0, 2000, 5000)\n",
    "\n",
    "def ecdf(data, x):\n",
    "    \"\"\" Compute ECDF \"\"\"\n",
    "    y = np.zeros_like(x)\n",
    "    \n",
    "    for i, x_i in enumerate(x):\n",
    "        y[i] = np.sum(data <= x_i)\n",
    "        \n",
    "    #Normalize by the number of samples\n",
    "    y/=len(data)\n",
    "    \n",
    "    return y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
